services:
  minio:
    image: minio/minio
    command: server /data --console-address :9001
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - ./volumes/minio-data:/data
    environment:
      - NO_PROXY=localhost,127.0.0.1,minio
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_REGION_NAME=us-east-1
      - TZ=UTC
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    networks:
      big-data-net:

  spark:
    image: apache/spark:3.5.6-scala2.12-java17-python3-r-ubuntu
    command: >
      /bin/bash -c
      "if compgen -G '/opt/spark/external-jars/*.jar' > /dev/null; then
         cp /opt/spark/external-jars/*.jar /opt/spark/jars/ 2>/dev/null || true;
       fi;
       exec /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    ports:
      - 8080:8080
      - 7077:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      # mount your pre-downloaded jars into a separate folder (do NOT overwrite /opt/spark/jars)
      - ./jars:/opt/spark/external-jars:ro
      - ./hive-conf:/opt/spark/conf
      - ./jars:/opt/spark/extra-jars
    environment:
      - TZ=UTC
      - SPARK_MASTER_HOST=spark
    networks:
      big-data-net:

  spark-worker:
    image: apache/spark:3.5.6-scala2.12-java17-python3-r-ubuntu
    command: >
      /bin/bash -c
      "if compgen -G '/opt/spark/external-jars/*.jar' > /dev/null; then
         cp /opt/spark/external-jars/*.jar /opt/spark/jars/ 2>/dev/null || true;
       fi;
       exec /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077"
    deploy:
      replicas: 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      - ./jars:/opt/spark/external-jars:ro
      - ./hive-conf:/opt/spark/conf
      - ./jars:/opt/spark/extra-jars
    environment:
      - TZ=UTC
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    networks:
      big-data-net:
  
  airflow:
    build: .
    image: airflow-image
    command: standalone
    environment:
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS=True
      - NO_PROXY=localhost,127.0.0.1,minio
      - TZ=UTC
    ports:
      - 8090:8080
    volumes:
      - airflow_data:/opt/airflow/data
      - airflow_cache:/opt/airflow/cache
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/jobs:/opt/airflow/jobs
      # provide jars to Airflow tasks (SparkSubmitOperator can reference /opt/airflow/external-jars)
      - ./jars:/opt/airflow/external-jars:ro
      - ./hive-conf:/opt/airflow/hive-conf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
    networks:
      big-data-net:

  metastore-db:
    image: postgres
    container_name: metastore-db
    hostname: metastore-db
    environment:
      - POSTGRES_DB=metastore_db
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
    volumes:
      - postgres_metastore_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore_db"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      big-data-net:

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    depends_on:
      - metastore-db
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      DB_HOST: metastore-db
      DB_PORT: 5432
      DB_NAME: metastore_db
      DB_USER: hive
      DB_PASSWORD: hive

      # Required so Hive loads JDBC + AWS S3 jars
      HIVE_AUX_JARS_PATH: /opt/hive/lib/*

      # Point warehouse to MinIO
      HIVE_CONF_hive_metastore_warehouse_dir: s3a://hudibuckettest/hive

      # MinIO S3A settings
      HADOOP_CONF_fs_s3a_endpoint: http://minio:9000
      HADOOP_CONF_fs_s3a_access_key: minioadmin
      HADOOP_CONF_fs_s3a_secret_key: minioadmin
      HADOOP_CONF_fs_s3a_path_style_access: "true"

    volumes:
      # Hive configs
      - ./hive-conf:/opt/hive/conf:ro

      # Postgres JDBC driver (must exist locally in ./jars/)
      - ./jars/postgresql-42.7.7.jar:/opt/hive/lib/postgresql-42.7.7.jar:ro

      # Hadoop AWS + AWS SDK bundles for S3A (download beforehand)
      - ./jars/hadoop-aws-3.3.6.jar:/opt/hive/lib/hadoop-aws-3.3.6.jar:ro
      - ./jars/aws-java-sdk-bundle-1.12.767.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.767.jar:ro
    networks:
      big-data-net:

volumes:
  airflow_data:
  airflow_cache:
  postgres_metastore_data:

networks:
  big-data-net:
    driver: bridge
