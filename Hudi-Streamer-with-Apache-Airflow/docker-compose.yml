services:
  minio:
    image: minio/minio
    command: server /data --console-address :9001
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - ./volumes/minio-data:/data
    environment:
      NO_PROXY: localhost,127.0.0.1,minio
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_REGION_NAME: us-east-1
      TZ: UTC
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    networks:
      big-data-net:

  spark:
    image: apache/spark:3.5.6-scala2.12-java17-python3-r-ubuntu
    command: >
      /bin/bash -c
      "exec /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master" 
    ports:
      - 8080:8080
      - 7077:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      # mount your pre-downloaded jars into a separate folder (do NOT overwrite /opt/spark/jars)
      - ./hive-conf:/opt/spark/conf
      - ./jars:/opt/spark/extra-jars
    environment:
      - TZ=UTC
      - SPARK_MASTER_HOST=spark
      - SPARK_SUBMIT_ARGS=--conf spark.jars=/opt/spark/external-jars/hudi-spark3.5-bundle_2.12-1.0.2.jar,/opt/spark/external-jars/hadoop-aws-3.3.6.jar,/opt/spark/external-jars/hadoop-common-3.3.6.jar,/opt/spark/external-jars/woodstox-core-6.7.0.jar,/opt/spark/external-jars/stax2-api-4.2.2.jar,/opt/spark/external-jars/commons-configuration2-2.12.0.jar,/opt/spark/external-jars/aws-java-sdk-bundle-1.12.262.jar
    networks:
      big-data-net:

  spark-worker:
    image: apache/spark:3.5.6-scala2.12-java17-python3-r-ubuntu
    command: >
      /bin/bash -c
      "exec /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077"
    deploy:
      replicas: 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
    volumes:
      - ./airflow/jobs:/opt/airflow/jobs
      - ./jars:/opt/spark/external-jars:ro
      - ./hive-conf:/opt/spark/conf
      - ./jars:/opt/spark/extra-jars
    environment:
      - TZ=UTC
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_SUBMIT_ARGS=--conf spark.jars=/opt/spark/external-jars/hudi-spark3.5-bundle_2.12-1.0.2.jar,/opt/spark/external-jars/hadoop-aws-3.3.6.jar,/opt/spark/external-jars/hadoop-common-3.3.6.jar,/opt/spark/external-jars/woodstox-core-6.7.0.jar,/opt/spark/external-jars/stax2-api-4.2.2.jar,/opt/spark/external-jars/commons-configuration2-2.12.0.jar,/opt/spark/external-jars/aws-java-sdk-bundle-1.12.262.jar
    networks:
      big-data-net:
  
  airflow:
    build: .
    image: airflow-image
    command: standalone
    environment:
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS=True
      - NO_PROXY=localhost,127.0.0.1,minio
      - TZ=UTC
    ports:
      - 8090:8080
    volumes:
      - airflow_data:/opt/airflow/data
      - airflow_cache:/opt/airflow/cache
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/jobs:/opt/airflow/jobs
      # provide jars to Airflow tasks (SparkSubmitOperator can reference /opt/airflow/external-jars)
      - ./jars:/opt/airflow/external-jars:ro
      - ./hive-conf:/opt/airflow/hive-conf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
    networks:
      big-data-net:

  metastore-db:
    image: postgres
    container_name: metastore-db
    hostname: metastore-db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=metastore_db
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
    volumes:
      - postgres_metastore_data:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore_db"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      big-data-net:

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    depends_on:
      - metastore-db
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      DB_HOST: metastore-db
      DB_PORT: 5432
      DB_NAME: metastore_db
      DB_USER: hive
      DB_PASSWORD: hive
      HIVE_CONF_datanucleus_schema_autoCreateAll: "true"
      HIVE_CONF_datanucleus_fixedDatastore: "false"
      HIVE_CONF_datanucleus_connectionPoolingType: "DBCP"
      HIVE_CONF_datanucleus_connectionRetryCount: "5"

      # Required so Hive loads JDBC + AWS S3 jars
      HIVE_AUX_JARS_PATH: /opt/hive/lib/*
      # Point warehouse to MinIO (This is still useful)
      HIVE_CONF_hive_metastore_warehouse_dir: s3a://hudibuckettest/hive
    volumes:
      # This volume mount now provides the core-site.xml
      - ./hive-conf:/opt/hive/conf:ro      
      # Other volumes remain the same
      - ./jars/postgresql-42.7.7.jar:/opt/hive/lib/postgresql-42.7.7.jar:ro
      - ./jars/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar:ro
      - ./jars/aws-java-sdk-bundle-1.12.262.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.262.jar:ro
    networks:
      big-data-net:

  trino:
    image: trinodb/trino:475
    container_name: trino
    ports:
      - "8087:8080"
    user: root
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/config:/etc/trino
      - ./trino-data:/data   
    depends_on:
      - hive-metastore
    networks:
      big-data-net:

  jupyterlab:
    build: ./jupyterlab
    container_name: jupyterlab
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    networks:
      - big-data-net
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=your-secure-password
      - GRANT_SUDO=yes
    user: root
    command: >
      bash -c "start-notebook.sh && 
      jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='${JUPYTER_TOKEN}'"

volumes:
  airflow_data:
  airflow_cache:
  postgres_metastore_data:

networks:
  big-data-net:
    driver: bridge
